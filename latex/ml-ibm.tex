\documentclass[9pt]{extarticle} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}          %
\usepackage{microtype}       
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{mathtools}        % More powerful math extensions
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{tikz}             % Drawing diagrams
\usepackage{booktabs}         % Nicer tabl
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage[dvipsnames]{xcolor}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage[nameinlink]{cleveref} % Smarter cross-referencing
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{hyperref}
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
\usetikzlibrary{arrows.meta, positioning}
\pagestyle{fancy}
\fancyhf{}
\lhead{Machine Learning with Python}
\rhead{\today}
\cfoot{\thepage}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}

\usepackage{tikz-cd}
\begin{document}
\begin{titlepage}
    Machine Learning with Python 
\end{titlepage}
\tableofcontents
\newpage
\section{Overview}

Machine learning is a subset of AI that uses 
computer algorithms that require feature engineering.  It 
teaches computers to learn from data and identify patterns and use 
them to make decisions without receiving explicit input 
from the user. There are three main types of machine learning models:
\begin{enumerate}
    \item Supervised learning models are trained on a known 
set of features and a target variable, to identify 
relationships which are then used for inference or 
forecasting on new data. For example: linear regression. 
    \item Unsupervised learning 
    models do not admit labelled feature-target style data, instead 
    they are trained on a set of variables which are all 
    considered features; the model finds relationships between these features. For 
    example: Principal Component Analysis.
    \item Reinforcement learning models simulate an AI agent interacting 
with its environment, they learn how to make decisions based on 
feedback from the environment. For example:
\end{enumerate}


The two focuses of supervised learning are regression and classification
One of the main types of unsupervised learning are clustering

\subsection{Machine Learning Lifecycle}

\begin{enumerate}
    \item \textbf{Problem Definition:} Clearly state the objective and desired outcome.
    
    \item \textbf{Data Collection:} Identify required data and its source.
    
    \item \textbf{Data Preparation:} Clean the data, handle missing values, normalize if necessary, engineer features, and perform exploratory data analysis. Split into training and testing sets.
    
    \item \textbf{Model Development:} Train the model, tune hyperparameters, and evaluate performance using appropriate metrics.
    
    \item \textbf{Deployment:} Integrate the trained model into a production environment.
\end{enumerate}


\subsection{Difference between a Data Scientist and AI Engineer} 

\begin{table}[h!]
\centering
\begin{tabular}{|p{3.5cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Aspect} & \textbf{Data Science} & \textbf{AI Engineering} \\
\hline
\textbf{Primary Use Cases} & Descriptive and predictive analytics (e.g., EDA, clustering, regression, classification) & Prescriptive and generative AI (e.g., optimisation, recommendation systems, intelligent assistants) \\
\hline
\textbf{Data Type Focus} & Primarily structured/tabular data, cleaned and preprocessed & Primarily unstructured data (text, images, audio, video), used at large scale \\
\hline
\textbf{Model Characteristics} & Narrow-scope ML models, smaller in size, domain-specific, faster to train & Foundation models, large-scale, general-purpose, high compute and data requirements \\
\hline
\textbf{Development Process} & Data-driven model development (feature engineering, training, validation) & Application of pre-trained models with prompt engineering, PEFT, and RAG frameworks \\
\hline
\end{tabular}
\caption{Key Differences Between Data Science and AI Engineering}
\end{table}


\subsection{Tools for Machine Learning}

Python has several modules that handle the different stages of the 
machine learning model development pipeline:
\begin{enumerate}
    \item Data preprocessing: \texttt{pysql}, \texttt{pandas}
    \item Exploratory data analysis: \texttt{pandas}, \texttt{numpy}, \texttt{matplotlib}
    \item Optimisation: \texttt{scipy}
    \item Implementation: \texttt{scikit-learn} (supervised and unsupervised methods), 
    \texttt{keras}, \texttt{pytorch} (deep learing)
\end{enumerate}

\subsection{Scikit-learn}

The basic syntax for using supervised learning models in \texttt{scikit-learn} follows a standard workflow:

\begin{enumerate}
    \item \textbf{Split the data:}
    \begin{verbatim}
x_train, X_test, y_train, y_test = train_test_split(X, y, test_size=...)
    \end{verbatim}

    \item \textbf{Import and initialise the model:}
    \begin{verbatim}
from sklearn import svm
model = svm.SVC(...)
    \end{verbatim}

    \item \textbf{Train the model:}
    \begin{verbatim}
model.fit(X_train, y_train)
    \end{verbatim}

    \item \textbf{Make predictions:}
    \begin{verbatim}
predictions = model.predict(X_test)
    \end{verbatim}

    \item \textbf{Evaluate performance:}
    Use a confusion matrix to assess classification accuracy:
    \begin{verbatim}
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)
    \end{verbatim}

    \item \textbf{Optional – Save the model:}
    You can serialise the trained model using \texttt{pickle}:
    \begin{verbatim}
import pickle
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)
    \end{verbatim}
    Whether this step is necessary depends on the context and industry practice.
\end{enumerate}

\section{Linear Regression}

Regression is a type of supervised learning model. It models a relationship between 
a continuous target variable and explanatory features.
\subsection{Linear Regression}


\subsubsection*{Definition}

Linear regression models the relationship between a response variable $Y$ and one or more features $X_1, \dots, X_p$. In the case of simple linear regression with one predictor $X$, the model assumes:

\[
Y \approx \beta_0 + \beta_1 X
\]
where $\beta_0$ is the intercept and $\beta_1$ is the slope.

\subsubsection*{Coefficient Estimation}

Given training data $(x_1, y_1), \dots, (x_n, y_n)$, the goal is to estimate the coefficients $\beta_0, \beta_1$ such that the fitted values are as close as possible to the observed values. This is done by minimizing a norm of the residual vector:

\[
\beta_{\text{min}} = \arg\min_{\beta \in \mathbb{R}^2} \| \mathbf{Y} - \mathbf{X} \beta \|_p^p
\]

For ordinary least squares (OLS), we use the $2$-norm ($p=2$), 
leading to the minimisation of the residual sum of squares (RSS). 
The OLS solution yields closed-form expressions for $\hat{\beta}_0$ and 
$\hat{\beta}_1$.
 For least absolute deviations (LAD), we take $p = 1$. 
 For Chebyshev or minimax regression, we take $p = \infty$, minimising the maximum residual.


\subsubsection*{Population vs Sample Regression}

In the real world we almost always do not have a maximal data set. 
This means that we do not have data for every single item in the population, 
all we can hope to obtain are samples. 
The fitted regression line from a 
sample \textit{estimates} the \emph{population regression line}:

\[
Y = \beta_0 + \beta_1 X + \varepsilon
\]


where $\varepsilon$ is the irreducible error due to unobserved factors. Since we cannot observe the full population, we fit the model on a sample and obtain estimates $\hat{\beta}_0, \hat{\beta}_1$.
However, since we only observe a sample, the estimates $\hat{\beta}$ vary
from sample to sample\footnote{
 The goal of statistical inference is to understand 
how close $\hat{\beta}$ is to the true $\beta$ and quantify this uncertainty.}.
The sample regression is thus an estimate of the population regression. 
If we had access to all population data, the OLS minimisation would yield the true $\beta_0$, $\beta_1$.


\subsubsection*{Sampling and the Central Limit Theorem}

Across repeated samples of size $n$, we obtain different estimates of $\beta$, say $\hat{\beta}^{(1)}, \dots, \hat{\beta}^{(n)}$. Their average converges to the true coefficient $\bar{\beta}$ as $n \to \infty$, due to the central limit theorem:

\[
\hat{\beta} - \bar{\beta} \xrightarrow{d} \mathcal{N}(0, \sigma_*)
\]

\subsubsection*{Standard Error and Inference}

The variance of the coefficient estimates across samples is the \emph{standard error}:

$$SE(\hat{\beta}_i) = f(\sigma^2, x_j, \bar{x})$$

where $\sigma^2$ is estimated from the data via:

\[
\hat{\sigma}^2 = \frac{RSS}{n - 2}
\]
The standard error quantifies the variability of the estimated coefficient 
under repeated sampling. If we observe a high standard error then it means that we do not see 
replicability of the relationship as we vary our sample, which may suggest that 
the relationship between the features and target is ephemeral and noisy, so 
any forecast or inference made using it will be unreliable.

\subsection{Multiple Linear Regression}

\subsubsection*{Model Definition}

Multiple linear regression generalises simple linear regression by modelling a response variable $Y$ as a linear combination of multiple predictors $X_1, \dots, X_p$:

\[
\hat{Y} = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \dots + \theta_p X_p = \mathbf{X}\boldsymbol{\theta}
\]

where:
\begin{itemize}
    \item $\mathbf{X}$ is the design matrix including a column of ones for the intercept,
    \item $\boldsymbol{\theta}$ is the parameter vector including $\theta_0$ (intercept) and $\theta_1, \dots, \theta_p$ (slopes).
\end{itemize}
For $p = 1$, the model defines a line as with the usual linear regression, 
for $p = 2$, the model defines a plane, 
for $p > 2$, the model defines a hyperplane in $\mathbb{R}^{p+1}$.

\subsubsection*{Estimation via Least Squares}

As before the $\boldsymbol{\theta}$ coefficients are the population 
coefficients, but in reality we can only estimate these. 
These are the sample coefficients $\boldsymbol{\hat{\theta}}$ which are obtained by minimising the 
mean squared error (MSE), but there is no closed form solution 
to this minimisation problem and so numerical methods are used.
For large datasets, 
iterative methods such as gradient descent can also be used to minimise MSE.

\subsubsection*{Prediction and Residuals}

Predictions for a new data point $\mathbf{x}$ are given by:

\[
\hat{y} = \mathbf{x}^\top \boldsymbol{\hat{\theta}}
\]
The residual error for observation $i$ is:

\[
\varepsilon_i = y_i - \hat{y}_i
\]
The squared residuals are used in minimising the MSE and finding the 
optimim sample coefficients.
\subsubsection*{Model Complexity and Overfitting}

Including more variables increases model flexibility but can lead to overfitting, 
where the model captures noise rather than signal. A balance is needed:
\begin{itemize}
    \item Remove redundant, highly correlated (collinear) variables.
    \item Choose variables that are interpretable, uncorrelated, and strongly related to $Y$.
\end{itemize}

\subsubsection*{Categorical Variables}

Categorical predictors must be encoded numerically:
\begin{itemize}
    \item Binary variables: encoded as $0/1$.
    \item Multiclass variables: use one-hot encoding (create a dummy variable for each class).
\end{itemize}

\subsubsection*{What-If Analysis}

The model can be used for counterfactual predictions by altering input features. However, what-if analysis may be invalid if:
\begin{itemize}
    \item Scenarios lie far outside the training data (extrapolation),
    \item Variables are collinear — changing one realistically requires changing another.
\end{itemize}

\subsection{Logistic Regression}

\subsubsection*{Model Definition}

Logistic regression models the probability that a 
binary outcome $Y \in \{0, 1\}$ occurs, 
given features $X_1, \dots, X_p$. The 
model assumes that the log-odds (logit) of the 
probability is a linear function of the input:

\[
\log\left( \frac{\mathbb{P}(Y = 1 \mid X)}{\mathbb{P}(Y = 0 \mid X)} \right) = \theta_0 + \theta_1 X_1 + \dots + \theta_p X_p = \mathbf{X}^\top \boldsymbol{\theta}
\]
Solving for the probability gives the sigmoid function:

\[
\hat{p} = \mathbb{P}(Y = 1 \mid X) = \sigma(\mathbf{X}^\top \boldsymbol{\theta}) = \frac{1}{1 + e^{-\mathbf{X}^\top \boldsymbol{\theta}}}
\]
This maps the linear combination of features to the 
interval $(0, 1)$, giving a probability.

\subsubsection*{Learning the Parameters}

To fit the model onto data we use maximum likelihood estimation to learn 
the $\boldsymbol{\theta}$ parameters. The optimal parameters $\boldsymbol{\hat{\theta}}$ are found by solving:

\[
\boldsymbol{\hat{\theta}} = \arg\min_{\boldsymbol{\theta}} \mathcal{J}(\boldsymbol{\theta})
\]

where $\mathcal{J}$ is negative log likelihood. 
This optimisation is typically performed using gradient descent or related numerical techniques.

\subsubsection*{Prediction and Decision Boundary}

The model returns a probability $\hat{p}$. To make a classification, we 
choose a threshold $\tau \in (0,1)$ (which is usually $\tau = 0.5$) and define:

\[
\hat{Y} = 
\begin{cases}
1 & \text{if } \hat{p} \geq \tau \\
0 & \text{otherwise}
\end{cases}
\]
This tells us that if $\mathbb{P}(Y=1|X)<0.5$ then $\hat{Y}$ is 
classified as belonging to class $0$. The threshold can be fine tuned 
to match reality, since most phenomena are not cleanly 50:50. For example 
fraud may occur one time out of 99 so using a 0.5 threshold would clearly 
be unwise since it would class everything as not-fraud.
\subsubsection*{Interpretability}

Each coefficient $\theta_j$ measures the change 
in the log-odds of $Y = 1$ for a unit increase in $X_j$, 
holding other variables fixed. Larger magnitudes indicate stronger influence. Importantly,
this allows us to conduct inference involving 
continuous change on a discrete classification problem.


\subsubsection*{Use Cases}

Logistic regression is used when:
\begin{itemize}
    \item The target variable is binary.
    \item Probabilities, not just classifications, are required.
    \item Model interpretability is important.
\end{itemize}

\subsection{Nonlinear and Polynomial Regression}

\subsubsection*{Motivation}

In many real-world datasets, the relationship between features $X$ and the 
target $Y$ is not well captured by a straight line. Nonlinear regression models are used when such relationships require curvature or more complex forms.

\subsubsection*{Nonlinear Regression: Definition}

Nonlinear regression models the relationship between $Y$ and the 
inputs $X_1, \dots, X_p$ via a nonlinear function:

\[
\hat{Y} = f(X_1, \dots, X_p; \boldsymbol{\theta})
\]

where $f$ is nonlinear in its parameters $\boldsymbol{\theta}$. Common forms include:
\begin{itemize}
    \item Exponential: $\hat{Y} = \theta_0 e^{\theta_1 X}$
    \item Logarithmic: $\hat{Y} = \theta_0 + \theta_1 \log(X)$
    \item Sinusoidal: $\hat{Y} = \theta_0 + \theta_1 \sin(\theta_2 X)$
\end{itemize}

\subsubsection*{Polynomial Regression}

Polynomial regression models $Y$ as an $n$-th degree polynomial in a single variable $X$:

\[
\hat{Y} = \theta_0 + \theta_1 X + \theta_2 X^2 + \dots + \theta_n X^n
\]
This model is nonlinear in the input but linear in the parameters, so it can be recast as:

\[
\hat{Y} = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \dots + \theta_n X_n
\]
where $X_k = X^k$, allowing the use of ordinary least squares.

\subsubsection*{Fitting and Optimisation}

For models linear in parameters (e.g., polynomial), parameters $\boldsymbol{\theta}$ can be fitted using standard linear regression:

\[
\boldsymbol{\hat{\theta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}
\]
For truly nonlinear models, where $f$ is nonlinear in $\boldsymbol{\theta}$, closed-form solutions do not exist. Instead, we minimise a loss (usually MSE) using iterative optimisation techniques, such as:

\[
\boldsymbol{\hat{\theta}} = \arg\min_{\boldsymbol{\theta}} \sum_{i=1}^n (y_i - f(x_i; \boldsymbol{\theta}))^2
\]
This is solved numerically using algorithms like gradient descent.

\subsubsection*{Model Selection and Overfitting}

Polynomial models of high degree can always perfectly fit any 
finite dataset (interpolate all points), but this leads to overfitting. 
Overfit models capture noise rather than signal.

\subsubsection*{Visual Model Selection}

To identify whether a nonlinear model is needed:
\begin{itemize}
    \item Plot scatter diagrams of $Y$ vs. $X_i$
    \item Look for nonlinearity (e.g., curvature, saturation, periodicity)
    \item Choose candidate functions: polynomial, exponential, logarithmic, etc.
\end{itemize}

\subsubsection*{Examples of Nonlinear Forms}

\begin{itemize}
    \item Exponential growth: GDP, investment returns
    \item Logarithmic growth: Diminishing returns in productivity
    \item Sinusoidal: Seasonal patterns, temperature cycles
\end{itemize}

\subsubsection*{Beyond Parametric Forms}

When no closed-form function is known, or when relationships are nonlinear 
and defy any known non-linear archetypes, meaning we cannot 
hope to represent the relationship via some standard paremetric, mathematical 
equation, we require more 
advanced methods such as
\begin{itemize}
    \item Decision trees
    \item Random forests
    \item Support vector machines
    \item Neural networks
    \item $k$-nearest neighbours
    \item Gradient boosting
\end{itemize}

These are non-parametric regressors that adapt to complex patterns 
without requiring an explicit functional form.

\section{Tree-based Methods}

\subsection{Decision Trees}


A \textbf{decision tree} is a 
supervised learning algorithm used for classification. 
It models decisions as a tree-like structure, where:

\begin{itemize}
    \item Internal nodes represent tests on input features,
    \item Branches represent outcomes of those tests,
    \item Leaf nodes assign a class (for classification) or a value (for regression).
\end{itemize}
The flow from root to leaf encodes a sequence of decisions that leads to a prediction.

\subsubsection*{Training: Recursive Partitioning}

A decision tree is trained by recursively partitioning the feature space to reduce uncertainty in the target variable. The training process proceeds as follows:

\begin{enumerate}
    \item Start with all training data at the root.
    \item At each node:
    \begin{itemize}
        \item Evaluate all possible feature-based splits,
        \item Select the split that maximally reduces impurity,
        \item Partition the data accordingly and assign subsets to new child nodes.
    \end{itemize}
    \item Repeat recursively for each child node.
    \item Stop splitting when:
    \begin{itemize}
        \item All samples in a node belong to the same class,
        \item No features remain,
        \item A stopping condition is met.
    \end{itemize}
\end{enumerate}
This process is known as \textbf{recursive binary splitting}.

\subsubsection*{Split Selection Criteria}

At each node, the goal is to select the feature and threshold that most reduces the impurity of the child nodes.

\textbf{1. Entropy (Information Gain)}:  
Entropy measures the randomness of the class distribution:
\[
\text{Entropy}(t) = -\sum_{k} p_k \log_2(p_k)
\]
where $p_k$ is the proportion of class $k$ in node $t$. The split that maximises the \textbf{Information Gain} is chosen:
\[
\text{IG} = \text{Entropy(parent)} - \sum_i \frac{n_i}{n} \cdot 
\text{Entropy(child}_i)
\]

\textbf{2. Gini Impurity}:  
An alternative impurity metric defined as:
\[
\text{Gini}(t) = 1 - \sum_{k} p_k^2
\]
Lower Gini implies purer nodes i.e more optimally split nodes. The best split minimises weighted Gini impurity.

\subsubsection*{Stopping and Pruning}

To avoid overfitting, tree growth must be controlled.
\begin{enumerate}
    \item \textbf{Pre-pruning (early stopping)}:  
Stop growing the tree when:
\begin{itemize}
    \item Maximum tree depth is reached,
    \item A minimum number of samples per node or per leaf is exceeded,
    \item A maximum number of leaves is reached.
\end{itemize}
\item \textbf{Post-pruning (reduction)}:  
Grow the full tree, then prune branches that do not improve performance on a validation set.

\end{enumerate}



Pruning simplifies the model, improves generalisation, and increases interpretability.

\subsubsection*{Interpretability}

Decision trees are highly interpretable models:
\begin{itemize}
    \item They can be visualised as flowcharts,
    \item Feature importance is revealed by the order of splits,
    \item Each path from root to leaf corresponds to a decision rule.
\end{itemize}

\subsection{Regression Trees}


A \textbf{regression tree} is a variant of the decision tree used for predicting continuous target variables rather than categorical labels. While classification trees assign inputs to discrete classes, regression trees predict a numerical value, typically by averaging the target values in a leaf node.

\subsubsection*{Problem Setup}

\begin{itemize}
    \item \textbf{Classification tree:} Target variable $Y$ is categorical; prediction is a class label.
    \item \textbf{Regression tree:} Target variable $Y$ is continuous; prediction is a real number $\hat{y}$.
\end{itemize}
The structure of the tree is the same as in classification: internal nodes represent decisions on input features, branches correspond to outcomes of those decisions, and leaves output predictions.

\subsubsection*{Training: Recursive Splitting}

The tree is built recursively by splitting the data to reduce variability in the target variable. At each node:

\begin{enumerate}
    \item For each feature, evaluate candidate thresholds to split the data into left and right subsets.
    \item For each candidate split, compute the \textbf{Mean Squared Error (MSE)}:
    \[
    \text{MSE}(t) = \frac{1}{n_t} \sum_{i=1}^{n_t} (y_i - \bar{y}_t)^2
    \]
    where $n_t$ is the number of observations in node $t$, and $\bar{y}_t$ is the mean of their target values.

    \item Compute the \textbf{weighted average MSE} of the split:
    \[
    \text{Split MSE} = \frac{n_L}{n} \text{MSE}_L + \frac{n_R}{n} \text{MSE}_R
    \]
    where $n_L$ and $n_R$ are the sample sizes in the left and right nodes respectively.

    \item Select the feature and threshold with the lowest split MSE.
    \item Recurse on child nodes until a stopping criterion is met.
\end{enumerate}

\subsubsection*{Prediction Rule}

At inference time, each data point traverses the tree until it reaches a leaf. The predicted value $\hat{y}$ is:

\[
\hat{y} = \frac{1}{n_{\text{leaf}}} \sum_{i \in \text{leaf}} y_i
\]
Other statistics (e.g., median) can be used if the target distribution is skewed, though the mean is most common due to computational efficiency.

\subsubsection*{Split Evaluation for Feature Types}

\begin{itemize}
    \item \textbf{Continuous features:} Trial thresholds $\alpha$ are defined as midpoints between sorted unique values. Exhaustive search yields the threshold that minimises weighted MSE.
    \item \textbf{Binary features:} Simply split on the two categories and compute the resulting split MSE.
    \item \textbf{Multi-class features:} Use strategies like one-vs-one or one-vs-all to reduce to binary splits, and evaluate each using the same MSE criterion.
\end{itemize}

\subsubsection*{Stopping Criteria and Pruning}

As with classification trees, growth can be controlled using:

\begin{itemize}
    \item Maximum depth,
    \item Minimum samples per node or per leaf,
    \item Maximum number of leaves.
\end{itemize}
\textbf{Post-pruning} may also be applied to remove splits that do not significantly reduce prediction error on a validation set.

\subsubsection*{Applications}

Regression trees are widely used in domains requiring interpretable predictions of continuous outcomes, such as:

\begin{itemize}
    \item Forecasting revenue,
    \item Predicting temperature or energy usage,
    \item Modelling medical risk scores or patient vitals,
    \item Estimating economic indicators.
\end{itemize}

\section{Support Vector Machines (SVM)}



Support Vector Machines (SVM) are supervised learning algorithms used for both classification and regression. The central idea is to represent data in a high-dimensional space and construct a separating hyperplane that best divides the data into classes (or fits a continuous target variable, in the case of regression).

\subsection{Linear SVM for Binary Classification}

\subsubsection*{Geometric Interpretation}

In the binary classification setting, SVM aims to find the optimal
 hyperplane that separates the data into two classes with the maximum margin. 
 Given two linearly separable classes, the separating hyperplane is defined as:

\[
\mathbf{w}^\top \mathbf{x} + b = 0
\]
where $\mathbf{w}$ is the normal vector to the hyperplane, and $b$ is the bias. The margin is the distance from the hyperplane to the nearest data point of 
either class. The points that lie closest to the hyperplane are called 
\textbf{support vectors}.

\subsubsection*{Optimisation Objective}

The optimisation problem is:

\[
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
\quad \text{subject to} \quad
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \quad \forall i
\]
This formulation ensures that all points are correctly classified w
ith a margin of at least 1, and $\|\mathbf{w}\|$ is minimised, thereby maximising the margin.

\subsubsection*{Soft Margin SVM}

In real-world data, perfect separation is mostly not possible. \textbf{Soft margin SVM} introduces slack variables to allow misclassifications:

\[
\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i
\quad \text{subject to} \quad
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\]
The parameter $C > 0$ controls the trade-off between maximising the margin and penalising misclassified points.

\subsection{Kernels and Nonlinear Decision Boundaries}

When data is not linearly separable in its original feature space, 
SVM employs the \textbf{kernel trick} to implicitly map data 
into a higher-dimensional space where a linear separation is possible.
Common kernels include:

\begin{itemize}
    \item \textbf{Linear kernel:} $K(\mathbf{x}, \mathbf{x}') = \mathbf{x}^\top \mathbf{x}'$
    \item \textbf{Polynomial kernel:} $K(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^\top \mathbf{x}' + c)^d$
    \item \textbf{Radial Basis Function (RBF) kernel:} $K(\mathbf{x}, \mathbf{x}') = \exp\left(-\gamma \|\mathbf{x} - \mathbf{x}'\|^2\right)$
    \item \textbf{Sigmoid kernel:} $K(\mathbf{x}, \mathbf{x}') = \tanh(\alpha \mathbf{x}^\top \mathbf{x}' + c)$
\end{itemize}
These enable SVM to construct complex, nonlinear decision boundaries while retaining convex optimisation.

\subsection{Support Vector Regression (SVR)}

SVM can also be adapted for regression tasks through \textbf{Support Vector Regression (SVR)}.

\subsubsection*{Idea}

Instead of trying to classify, SVR aims to fit a function that deviates from the observed targets by no more than a user-defined threshold $\varepsilon$. The region within which no penalty is applied is called the \textbf{$\varepsilon$-tube}.

\subsubsection*{Optimisation Formulation}

The optimisation objective becomes:

\[
\min_{\mathbf{w}, b, \xi, \xi^*} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*)
\]
subject to:

\[
\begin{aligned}
y_i - \mathbf{w}^\top \mathbf{x}_i - b &\leq \varepsilon + \xi_i \\
\mathbf{w}^\top \mathbf{x}_i + b - y_i &\leq \varepsilon + \xi_i^* \\
\xi_i, \xi_i^* &\geq 0
\end{aligned}
\]
Here, $\varepsilon$ controls the tolerance zone, and $C$ balances flatness vs tolerance of errors.

\subsection{Advantages and Limitations}

\textbf{Advantages:}
\begin{itemize}
    \item Effective in high-dimensional spaces.
    \item Robust to overfitting (especially with margin control).
    \item Flexible via kernel functions for nonlinear problems.
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Training can be slow on large datasets.
    \item Sensitive to noisy data and overlapping classes.
    \item Performance depends heavily on kernel and hyperparameter selection.
\end{itemize}

\subsection{Use Cases}

SVMs are widely used in:

\begin{itemize}
    \item Image classification (e.g., digit recognition)
    \item Spam filtering
    \item Text and sentiment analysis
    \item Speech recognition and anomaly detection
    \item Bioinformatics and pattern recognition
\end{itemize}

\section{K-Nearest Neighbours (KNN)}


K-Nearest Neighbours (KNN) is a supervised learning algorithm used for both classification and regression. It operates on the assumption that data points close to one another in the feature space are likely to share similar labels. Unlike other models, KNN is a \textbf{lazy learner}: it does not build an explicit model during training but instead stores the dataset and performs computation at prediction time.

\subsection{Algorithmic Procedure}

Given a value of $k$, and a new data point (the \textit{query}), the KNN algorithm proceeds as follows:

\begin{enumerate}
    \item Compute the distance from the query point to all labelled training data.
    \item Select the $k$ nearest data points.
    \item \textbf{Classification:} predict the class that occurs most frequently among the $k$ neighbours (majority vote).
    \item \textbf{Regression:} predict the average (or median) of the target values of the $k$ neighbours.
\end{enumerate}

Common distance metrics include Euclidean, Manhattan, and Chebyshev distance (the 
$p$-norms with $p=2,1,\infty$). The choice of metric affects the neighbourhood 
geometry and, hence, prediction accuracy.

\subsection{Decision Boundary}

The \textbf{decision boundary} of a KNN classifier is shaped by the spatial distribution of training data. For low values of $k$, the boundary is highly irregular, following the noise in the data. As $k$ increases, the boundary becomes smoother, leading to more generalised predictions.

\subsection{Hyperparameter Tuning and Bias–Variance Trade-off}

\begin{itemize}
    \item \textbf{Small $k$:} Low bias, high variance. The model is highly sensitive to noise (overfitting).
    \item \textbf{Large $k$:} High bias, low variance. The model smooths over fine details (underfitting).
\end{itemize}
The optimal $k$ is usually found via cross-validation.

\subsection{Feature Scaling and Relevance}

KNN is sensitive to the scale of features because distance metrics are directly affected by magnitude. Features should be standardised (e.g., via z-score normalisation) before applying KNN. Furthermore, including irrelevant or redundant features can degrade performance:

\begin{itemize}
    \item Noisy features increase the required $k$ to achieve stability.
    \item Irrelevant features dilute meaningful distances.
    \item Domain knowledge is crucial in selecting relevant features.
\end{itemize}

\subsection{Weighted KNN}

To address issues such as class imbalance or outliers, \textbf{distance-weighted KNN} assigns weights to neighbours inversely proportional to their distance from the query point. Closer points have greater influence:

\[
w_i = \frac{1}{d(x, x_i)^2}
\]
where $x$ is the query point and $x_i$ is a neighbour.

\subsection{Regression with KNN}

In KNN regression, predictions are made by averaging the target values of the $k$ nearest neighbours:

\[
\hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i
\]

This can be modified to a weighted average using distance-based weights to reduce the influence of distant neighbours.

\subsection{Advantages and Limitations}

\textbf{Advantages:}
\begin{itemize}
    \item Simple, intuitive, and easy to implement.
    \item Naturally handles multi-class classification.
    \item No training phase; all computation is deferred until prediction.
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Computationally expensive at prediction time.
    \item Sensitive to feature scaling and irrelevant features.
    \item Poor performance on high-dimensional data due to the curse of dimensionality.
\end{itemize}

\subsection{Use Cases}

KNN is well-suited to:

\begin{itemize}
    \item Pattern recognition (e.g., handwritten digit classification)
    \item Recommender systems
    \item Medical diagnosis (e.g., classifying patient symptoms)
    \item Anomaly detection
\end{itemize}
\section{Bias--Variance Tradeoff and Ensemble Methods}

\subsection{Bias and Variance: An Intuitive Framework}

The concepts of \textbf{bias} and \textbf{variance} describe two key sources of error in supervised learning models.

\begin{itemize}
    \item \textbf{Bias} measures the error introduced by approximating a complex real-world function with a simpler model. High bias models tend to underfit the data.
    \item \textbf{Variance} measures the model's sensitivity to fluctuations in the training data. High variance models tend to overfit.
\end{itemize}


\subsection{Prediction Bias and Variance}

\begin{itemize}
    \item \textbf{Prediction bias} quantifies the average deviation of model predictions from the true target values. A model with zero bias produces predictions that are, on average, correct.
    \item \textbf{Prediction variance} quantifies how much 
    model predictions vary across different samples. 
    High variance reflects a model that overfits on ephemera.
\end{itemize}


\subsection{The Tradeoff and Model Complexity}

As model complexity increases:
\begin{itemize}
    \item Bias decreases, since the model can fit training data more closely.
    \item Variance increases, since the model becomes sensitive to fluctuations and noise in the data.
\end{itemize}

The \textbf{bias–variance tradeoff} captures the tension between underfitting and overfitting. The optimal model lies at the point where the sum of squared bias and variance is minimized, balancing generalization and flexibility.

\subsection{Weak vs Strong Learners}

\begin{itemize}
    \item A \textbf{weak learner} is a model that performs slightly better than random guessing. It typically exhibits \textit{high bias} and \textit{low variance}.
    \item A \textbf{strong learner} performs significantly better but often exhibits \textit{low bias} and \textit{high variance}.
\end{itemize}

Decision trees are frequently used as weak learners in ensemble methods due to their tunable bias–variance properties.

\subsection{Ensemble Methods}

\textbf{Ensemble learning} combines multiple models (often weak learners) to produce a stronger predictor. Two primary techniques are \textbf{bagging} and \textbf{boosting}, which address bias and variance in different ways.

\subsubsection{Bagging (Bootstrap Aggregating)}

Bagging reduces variance without substantially increasing bias.

\begin{itemize}
    \item It trains multiple base models (e.g., decision trees) on \textit{bootstrap samples} of the data.
    \item The final prediction is the average (for regression) or majority vote (for classification) of the base models.
\end{itemize}

Bagging benefits from the law of large numbers: averaging multiple high-variance models leads to a lower-variance aggregate.

\paragraph{Random Forests}

Random Forests are a popular implementation of bagging:
\begin{itemize}
    \item A large number of decision trees are trained on different bootstrap samples.
    \item During training, each node considers only a random subset of features.
    \item Final predictions are aggregated across trees.
\end{itemize}

This strategy further decorrelates the trees and reduces overfitting.

\subsubsection{Boosting}

Boosting reduces bias by sequentially training models that correct the errors of their predecessors.

\begin{itemize}
    \item Each weak learner is trained on a modified dataset, with higher weights given to previously misclassified examples.
    \item The final model is a weighted sum of all learners.
\end{itemize}

Boosting increases model complexity iteratively, making it capable of approximating complex functions.

\paragraph{Popular Boosting Algorithms}
\begin{itemize}
    \item \textbf{AdaBoost:} Emphasizes misclassified examples by increasing their weights.
    \item \textbf{Gradient Boosting:} Models the residuals of prior models using gradient descent.
    \item \textbf{XGBoost:} An efficient and scalable implementation of gradient boosting.
\end{itemize}

\subsection{Comparison of Bagging and Boosting}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Aspect} & \textbf{Bagging} & \textbf{Boosting} \\
\hline
Goal & Reduce variance & Reduce bias \\
Model training & Parallel & Sequential \\
Data sampling & Bootstrap (random) & Weighted (focused on errors) \\
Learner type & High variance, low bias & Low variance, high bias \\
Overfitting risk & Lower & Higher (if not regularized) \\
Example & Random Forests & AdaBoost, XGBoost \\
\hline
\end{tabular}
\end{table}


\section{Unsupervised Learning}

Unlike supervised learning, \textbf{unsupervised learning} deals with data that has no labels. The objective is to uncover hidden patterns, structures, or relationships within the data without any prior supervision. One of the most commonly used unsupervised learning algorithms is \textbf{K-Means Clustering}.

\subsection{K-Means Clustering}

K-Means is an iterative, centroid-based clustering algorithm that partitions a dataset into $k$ non-overlapping clusters. It groups data points based on their similarity, measured via distance to cluster centroids.

\subsubsection{Objective}

The aim of K-Means is to minimise the \textit{within-cluster sum of squares (WCSS)}, defined as:

\[
\sum_{i=1}^k \sum_{\mathbf{x} \in C_i} \lVert \mathbf{x} - \boldsymbol{\mu}_i \rVert^2
\]

where $C_i$ is the $i$-th cluster and $\boldsymbol{\mu}_i$ is its centroid (mean).

\subsubsection{Algorithm Steps}

\begin{enumerate}
    \item Choose the number of clusters $k$.
    \item Initialise $k$ centroids randomly.
    \item Assign each data point to the nearest centroid using a distance metric (typically Euclidean).
    \item Recalculate the centroids as the mean of all points assigned to each cluster.
    \item Repeat steps 3 and 4 until convergence (i.e., centroids no longer move or a maximum number of iterations is reached).
\end{enumerate}

\subsubsection{Convergence and Performance}

K-Means typically converges quickly and scales well to large datasets. However, performance is highly sensitive to:
\begin{itemize}
    \item Initialisation of centroids
    \item The value of $k$
    \item Presence of noise or outliers
    \item Cluster shape and balance
\end{itemize}

\subsubsection{Assumptions and Limitations}

\begin{itemize}
    \item Assumes clusters are convex and roughly equal in size.
    \item Sensitive to outliers and noise, as variance-based measures are easily distorted.
    \item Performs poorly when clusters differ significantly in size or density.
    \item Not suitable for discovering non-convex clusters.
\end{itemize}

\subsubsection{Visual Example and Convergence}

During convergence, centroids move to minimise intra-cluster distances. For example, in a dataset with two circular clusters, initial centroids placed randomly may gradually shift towards the true cluster centres over several iterations. Convergence is reached when the assignments no longer change.

\subsubsection{Choosing the Number of Clusters}

Selecting an appropriate $k$ is non-trivial, especially in high-dimensional space. Common heuristic methods include:

\begin{itemize}
    \item \textbf{Elbow Method:} Plot WCSS against $k$ and look for the “elbow” point where additional clusters yield diminishing returns.
    \item \textbf{Silhouette Score:} Measures cohesion vs separation. Values close to 1 indicate well-separated clusters.
    \item \textbf{Davies-Bouldin Index:} Lower values indicate better clustering, based on intra- and inter-cluster distances.
\end{itemize}

\subsubsection{Imbalanced and Overlapping Clusters}

K-Means struggles with:
\begin{itemize}
    \item \textbf{Imbalanced clusters:} Smaller clusters may be absorbed into larger ones.
    \item \textbf{Overlapping clusters:} When clusters are not well-separated, K-Means can mislabel data points.
    \item \textbf{Incorrect $k$:} If $k$ is too small or too large, the resulting clustering may not reflect the true structure of the data.
\end{itemize}


\subsection{DBSCAN and HDBSCAN}

\textbf{DBSCAN} (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that identifies clusters as dense regions of points separated by areas of lower density. Unlike centroid-based methods (e.g., K-Means), DBSCAN does not assume convex cluster shapes and does not require the user to specify the number of clusters in advance.

\subsubsection{Core Concepts}

DBSCAN requires two key parameters:
\begin{itemize}
    \item $\varepsilon$ (epsilon): the radius defining a neighbourhood around a point.
    \item \texttt{minPts}: the minimum number of points required to form a dense region.
\end{itemize}

Based on these, each point is classified as one of the following:
\begin{itemize}
    \item \textbf{Core Point:} A point with at least \texttt{minPts} points (including itself) within its $\varepsilon$-neighbourhood.
    \item \textbf{Border Point:} A point that lies within the $\varepsilon$-neighbourhood of a core point but is not itself a core point.
    \item \textbf{Noise Point:} A point that does not belong to the $\varepsilon$-neighbourhood of any core point.
\end{itemize}

\subsubsection{How DBSCAN Works}

\begin{enumerate}
    \item Select an arbitrary unvisited point.
    \item Retrieve its $\varepsilon$-neighbourhood.
    \item If it contains at least \texttt{minPts}, start a new cluster.
    \item Expand the cluster by recursively visiting all $\varepsilon$-neighbours of core points.
    \item Label remaining unvisited points as noise.
\end{enumerate}

This approach allows DBSCAN to:
\begin{itemize}
    \item Discover clusters of arbitrary shape and size.
    \item Identify and exclude outliers or noisy data points.
    \item Avoid the need to predefine the number of clusters.
\end{itemize}

However, DBSCAN has limitations:
\begin{itemize}
    \item Sensitive to the choice of $\varepsilon$ and \texttt{minPts}.
    \item Struggles with datasets containing varying densities.
\end{itemize}

\subsubsection{Example}

In the case of two interlocking half-moon shapes (a common synthetic dataset), DBSCAN successfully separates the shapes using density rather than geometric assumptions. Core points within each moon shape form clusters, border points are appended, and outliers are excluded.

\subsubsection{HDBSCAN: Hierarchical DBSCAN}

\textbf{HDBSCAN} (Hierarchical DBSCAN) is an extension of DBSCAN that improves upon its weaknesses. It eliminates the need to set $\varepsilon$ manually and adapts to varying local densities.

Key features include:
\begin{itemize}
    \item Builds a hierarchy of clusters by varying the density threshold.
    \item Selects the most stable clusters via \textit{persistence}, a measure of how long a cluster exists as density thresholds change.
    \item More robust to noise and better at identifying meaningful clusters of different densities.
\end{itemize}

\subsubsection{How HDBSCAN Works}

\begin{enumerate}
    \item Each point starts as its own cluster.
    \item A mutual reachability graph is built, using distance scaled by local density.
    \item Clusters are merged in a hierarchical fashion as the density threshold decreases.
    \item A condensed cluster tree is produced by pruning low-stability branches.
\end{enumerate}

\subsubsection{DBSCAN vs HDBSCAN}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspect} & \textbf{DBSCAN} & \textbf{HDBSCAN} \\
\hline
Input parameters & $\varepsilon$, \texttt{minPts} & \texttt{min\_cluster\_size}, \texttt{min\_samples} \\
Cluster shapes & Arbitrary & Arbitrary \\
Density adaptation & Fixed & Variable (adaptive) \\
Handles noise & Yes & Yes (better) \\
Requires $\varepsilon$ & Yes & No \\
Scalability & Good & Moderate \\
\hline
\end{tabular}
\end{center}

\subsubsection{Use Case: Canadian Museums}

When applied to spatial data such as latitudes and longitudes of Canadian museums, DBSCAN was able to detect natural geographic clusters but struggled in high-density urban areas by lumping many points together. HDBSCAN, with adaptive neighbourhood sizing, successfully identified finer structures and revealed multiple clusters within dense metropolitan regions.

\subsection{Dimensionality Reduction}

Dimensionality reduction refers to techniques that reduce the number of features in a dataset while preserving essential information. High-dimensional data can be challenging to visualise and computationally expensive to process. These methods aim to project the data into a lower-dimensional space that captures the structure, variance, or similarity relationships inherent in the original data.

Popular dimensionality reduction algorithms include:
\begin{itemize}
    \item Principal Component Analysis (PCA)
    \item t-Distributed Stochastic Neighbour Embedding (t-SNE)
    \item Uniform Manifold Approximation and Projection (UMAP)
\end{itemize}

\subsubsection{Principal Component Analysis (PCA)}

Principal Component Analysis is a linear technique that assumes the features in a dataset are linearly correlated. PCA projects the data onto a new coordinate system defined by orthogonal directions called \textit{principal components}, which are ordered by the amount of variance they capture from the original data.

\begin{itemize}
    \item The first principal component captures the direction of maximum variance.
    \item Subsequent components are orthogonal and capture decreasing amounts of variance.
    \item PCA transforms the original features into a reduced set of uncorrelated variables.
\end{itemize}

PCA is effective when the structure in the data is linear and often used for noise reduction and feature compression.

\subsubsection{t-Distributed Stochastic Neighbour Embedding (t-SNE)}

t-SNE is a non-linear dimensionality reduction algorithm particularly suited for high-dimensional data such as text and image features.

\begin{itemize}
    \item It maps the data into two or three dimensions by preserving the local similarity between points.
    \item Points that are close in the high-dimensional space remain close in the low-dimensional embedding.
    \item Distant points are given less emphasis in the optimisation.
\end{itemize}

Despite its ability to reveal hidden clusters, t-SNE has limitations:
\begin{itemize}
    \item High computational cost and poor scalability.
    \item Sensitivity to hyperparameters such as perplexity and learning rate.
\end{itemize}

\subsubsection{Uniform Manifold Approximation and Projection (UMAP)}

UMAP is a more recent non-linear method that improves upon the limitations of t-SNE by incorporating manifold learning.

\begin{itemize}
    \item It constructs a graph in the original space assuming that the data lies on a low-dimensional manifold.
    \item A second graph is constructed in low dimensions to preserve the local and global structure of the original data.
    \item UMAP generally scales better and produces more meaningful embeddings for large and complex datasets.
\end{itemize}

\subsubsection{Comparison Example}

To illustrate the comparative behaviour of these algorithms, consider a synthetic dataset comprising four Gaussian blobs generated in three dimensions. The blobs vary slightly in their standard deviations, resulting in slight overlaps between some clusters.

\textbf{PCA:} Since the blobs are linearly separable, PCA performs well, projecting the clusters into 2D while retaining the primary directions of variance.

\textbf{t-SNE:} The algorithm identifies four well-separated clusters but introduces minor misclassifications, particularly where original clusters overlap. t-SNE is effective at revealing local structure but can distort global relationships.

\textbf{UMAP:} UMAP recovers most of the cluster structure while preserving both local and global features. It outperforms t-SNE in this case by better maintaining the density structure of the original data.

\section{Evaluating Models}
\subsection{Supervised Learning}
\subsubsection{Classification}

Supervised learning evaluation measures how effectively a machine learning model can predict outcomes on unseen data. This process is crucial for assessing a model's generalisation capability. Evaluation occurs during both the training and testing phases, ensuring that the model does not merely memorise the training data but captures patterns that extend to new inputs.

A standard approach is the \textit{train-test split}, where the dataset is divided into two subsets:
\begin{itemize}
    \item \textbf{Training set:} Typically 70--80\% of the data, used to train the model.
    \item \textbf{Test set:} The remaining 20--30\%, used to evaluate model performance on unseen data.
\end{itemize}

In classification tasks, the model predicts categorical labels. Evaluation metrics compare these predicted labels to the true labels to assess performance.

\paragraph{Accuracy}

Accuracy is the proportion of correctly predicted instances:
\[
\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}
\]

\paragraph{Confusion Matrix}

A confusion matrix provides a more detailed breakdown:
\begin{itemize}
    \item \textbf{True Positives (TP):} Correctly predicted positive instances.
    \item \textbf{True Negatives (TN):} Correctly predicted negative instances.
    \item \textbf{False Positives (FP):} Incorrectly predicted as positive.
    \item \textbf{False Negatives (FN):} Incorrectly predicted as negative.
\end{itemize}

\paragraph{Precision}

Precision is the ratio of true positives to all predicted positives:
\[
\text{Precision} = \frac{\text{TP}}{\text{TP + FP}}
\]
This metric is important when the cost of false positives is high (e.g.\ in recommendation systems).

\paragraph{Recall}

Recall measures the ability of the model to detect actual positives:
\[
\text{Recall} = \frac{\text{TP}}{\text{TP + FN}}
\]
It is critical when the cost of false negatives is high (e.g.\ in medical diagnostics).

\paragraph{F1 Score}

The F1 score is the harmonic mean of precision and recall:
\[
\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision + Recall}}
\]
It is useful when precision and recall are equally important.

\paragraph{Example: Iris Classification}

Using the \texttt{KNeighborsClassifier} on the \texttt{Iris} dataset, one can evaluate classification performance with a confusion matrix and associated metrics for each class: \texttt{setosa}, \texttt{versicolor}, and \texttt{virginica}. A heat map of the confusion matrix visualises correct and incorrect predictions. In a well-performing model, diagonal entries (correct classifications) will dominate.

\paragraph{Class-Specific Metrics}

Precision, recall, and F1 scores can be computed per class. A weighted average accounts for class imbalance by weighting each metric according to the number of samples in that class.



\subsubsection{Regression}

Regression models aim to predict continuous numerical values. However, these predictions are not exact and typically involve some degree of error. Evaluation of regression models is therefore essential to assess how well the model approximates true outcomes.

Let us consider an example: predicting final exam scores based on midterm results. A regression line is fit to the data, and the differences between the actual grades and the predicted values form the model's \textit{errors}. These errors, also called residuals, quantify the deviation between the fitted model and the observed data.

Formally, for a dataset $\{(x_i, y_i)\}_{i=1}^n$, with predictions $\hat{y}_i$ from the model, several error-based metrics are used to quantify the quality of fit.

\paragraph{Mean Absolute Error (MAE)}

The MAE measures the average magnitude of the errors:
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |\hat{y}_i - y_i|
\]

\paragraph{Mean Squared Error (MSE)}

The MSE penalises larger errors more heavily by squaring the differences:
\[
\text{MSE} = \frac{1}{n - p} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
\]
where $p$ is the number of model parameters.

\paragraph{Root Mean Squared Error (RMSE)}

The RMSE is the square root of the MSE and is often preferred because it shares the same units as the target variable:
\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{ \frac{1}{n - p} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 }
\]

\paragraph{Coefficient of Determination ($R^2$)}

The $R^2$ score measures the proportion of variance in the target variable explained by the model:
\[
R^2 = 1 - \frac{\sum_{i=1}^{n} (\hat{y}_i - y_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]
An $R^2$ value of 1 indicates perfect prediction, while 0 means the model performs no better than simply predicting the mean $\bar{y}$ of the observed data. Negative values imply that the model performs worse than the mean-value predictor.

\paragraph{Explained Variance}

Explained variance quantifies the amount of variation captured by the model:
\[
\text{Explained Variance} = \frac{\sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]
It is closely related to $R^2$ and equals $1$ for perfect predictions.

\paragraph{Interpreting Metrics}

\begin{itemize}
    \item A lower MAE, MSE, and RMSE indicate smaller prediction errors.
    \item A higher $R^2$ or explained variance reflects a better model fit.
    \item RMSE is sensitive to outliers due to the squaring operation.
    \item MAE is more robust to outliers.
    \item $R^2$ may be misleading for non-linear relationships.
\end{itemize}

\paragraph{Example: Effect of Transformations}

Consider fitting a linear model to a target variable following a log-normal distribution. Transformations such as the logarithm or Box-Cox improve linearity. Visually, this causes the data to cluster more closely around the best-fit line. Consequently, error metrics (MAE, MSE, RMSE) decrease, and $R^2$ increases, reflecting improved model performance.


\subsection{Unsupervised Learning}

Evaluating unsupervised learning models presents distinct challenges, as there are no predefined labels or ground truths to compare against. Unlike supervised learning, the goal is to uncover hidden structures or patterns within the data. Evaluation focuses on the coherence, separability, and stability of these patterns.

\paragraph{Challenges and Importance}

Without target labels, unsupervised model evaluation relies heavily on heuristic metrics, domain-specific insights, and qualitative analysis. A good model should group similar points into coherent clusters and preserve the essential structure of the data, even under perturbations. \textit{Stability} refers to a model's ability to produce consistent clusters across variations in the dataset.

Effective evaluation therefore involves a multifaceted approach combining:
\begin{itemize}
    \item Internal evaluation metrics
    \item External evaluation metrics (when labels are available)
    \item Visual inspection and dimensionality reduction
    \item Domain expertise
    \item Model stability analysis
\end{itemize}

\subsubsection*{Evaluating Clustering Models}

Clustering aims to partition data into homogeneous groups. Its evaluation typically relies on either internal or external metrics.

\paragraph{Internal Evaluation Metrics}

These metrics assess clustering quality based solely on the data and the resulting cluster structure:
\begin{itemize}
    \item \textbf{Silhouette Score:}
    \[
    s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} \in [-1, 1]
    \]
    where $a(i)$ is the average intra-cluster distance and $b(i)$ is the lowest average inter-cluster distance. Higher scores indicate better separation and cohesion.
    
    \item \textbf{Davies-Bouldin Index (DBI):} Measures average similarity between clusters:
    \[
    \text{DBI} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \ne i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)
    \]
    where $\sigma_i$ is the average distance of points in cluster $i$ to its centroid $c_i$, and $d$ is the distance between centroids. Lower values indicate better clustering.
    
    \item \textbf{Inertia:} Also called within-cluster sum of squares:
    \[
    \text{Inertia} = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2
    \]
    where $\mu_i$ is the centroid of cluster $C_i$. Lower values imply tighter clusters.
\end{itemize}

\paragraph{External Evaluation Metrics}

When ground-truth labels are available, these metrics compare predicted cluster assignments to true labels:
\begin{itemize}
    \item \textbf{Adjusted Rand Index (ARI):} Measures similarity between clustering and ground truth:
    \[
    \text{ARI} \in [-1, 1], \quad 1 = \text{perfect match}, \quad 0 = \text{random}, \quad < 0 = \text{worse than random}
    \]
    
    \item \textbf{Normalized Mutual Information (NMI):}
    \[
    \text{NMI}(U, V) = \frac{2 \cdot I(U; V)}{H(U) + H(V)} \in [0, 1]
    \]
    where $I$ is mutual information and $H$ is entropy. Measures shared information between cluster labels and true classes.
    
    \item \textbf{Fowlkes–Mallows Index (FMI):}
    \[
    \text{FMI} = \sqrt{\frac{TP}{TP + FP} \cdot \frac{TP}{TP + FN}}
    \]
    A geometric mean of precision and recall for clustering results.
\end{itemize}

\paragraph{Visualization and Dimensionality Reduction}

Scatter plots of dimensionality-reduced data help qualitatively assess clustering. Techniques include:
\begin{itemize}
    \item \textbf{PCA:} Projects data onto orthogonal principal components. The \textit{explained variance ratio} indicates how much information is retained in each component.
    \item \textbf{t-SNE and UMAP:} Nonlinear methods that preserve local structure. Evaluation includes:
        \begin{itemize}
            \item \textit{Reconstruction Error:} Measures how well the original data can be recovered.
            \item \textit{Neighborhood Preservation:} Assesses retention of local relationships.
        \end{itemize}
\end{itemize}

\paragraph{Case Study: K-Means Clustering}

Simulated blob data with varying separability is clustered using K-Means. The following patterns emerge:
\begin{itemize}
    \item Clear separation results in high silhouette scores (e.g., $s = 0.84$) and low DBI ($= 0.22$).
    \item Poor separation yields lower silhouette scores ($\approx 0.58$) and higher DBI.
    \item Increasing $k$ reduces inertia but may lead to over-segmentation.
\end{itemize}





\section{Model Generalisability}
\subsection{Model Validation}

Model validation is a critical component in the development of machine learning models. Its primary goal is to ensure that a model generalizes well to unseen data, rather than merely fitting the training data. This process safeguards against overfitting, particularly during the tuning of hyperparameters, and helps assess how well a model will perform in practical, real-world scenarios.

\subsubsection*{Motivation}

Many machine learning models come with hyperparameters—settings that must be chosen prior to training and which directly affect model performance. If one naively tunes hyperparameters based on performance on the test set, this effectively fits the model to the test data, compromising its role as an unbiased benchmark. This practice, known as \textit{data snooping} or a form of \textit{data leakage}, leads to overfitting and invalidates test set performance as an estimator of real-world generalisation.

\subsubsection*{Train-Validation-Test Split}

To address this, model validation requires a disciplined separation of data:
\begin{itemize}
    \item \textbf{Training Set:} Used to fit the model and update its internal parameters.
    \item \textbf{Validation Set:} A holdout set from the training data used to tune hyperparameters and assess model performance during development.
    \item \textbf{Test Set:} A final, untouched set used solely for evaluating the generalisation ability of the final model.
\end{itemize}

This three-way split ensures that hyperparameter optimisation remains decoupled from final evaluation, avoiding the pitfalls of data leakage.

\subsubsection*{Cross-Validation}

Using a single validation set can lead to instability—model performance may vary significantly depending on which subset is chosen. Furthermore, in data-constrained environments, reserving a separate validation set may reduce the amount of data available for training, leading to biased estimates of model performance.

\textbf{K-fold cross-validation} provides a principled solution. The data is split into $K$ equally sized folds. For each fold:
\begin{enumerate}
    \item Train the model on the remaining $K-1$ folds.
    \item Evaluate it on the $k^\text{th}$ fold (the validation fold).
\end{enumerate}
This process is repeated $K$ times, with each fold serving as the validation set once. The model’s performance is then averaged over all $K$ trials. This ensures that:
\begin{itemize}
    \item Every data point is used for both training and validation.
    \item Variance from arbitrary splits is reduced.
    \item Hyperparameter selection becomes more robust.
\end{itemize}

Typical values for $K$ range from 5 to 10, striking a balance between computational efficiency and robustness.

\subsubsection*{Stratified Cross-Validation and Skewed Targets}

In classification, datasets often exhibit class imbalance—where some classes dominate in frequency. This can bias evaluation. \textbf{Stratified cross-validation} preserves class proportions across folds, ensuring a fairer evaluation across all classes.

In regression, the analogous issue arises when the target variable is highly skewed. Many regression models implicitly assume a normally distributed target. In such cases, one can apply transformations to the target variable:
\begin{itemize}
    \item \textbf{Logarithmic Transformation:} Useful when the target exhibits exponential growth.
    \item \textbf{Box-Cox Transformation:} A family of power transformations that can normalise skewed distributions.
\end{itemize}

These transformations can improve linearity and model fit, thereby enhancing predictive accuracy and stability.


\subsection{Regularisation in Regression and Classification}

Regularisation is a technique used to prevent overfitting in both regression and classification models. Overfitting occurs when a model learns the noise in the training data rather than the underlying pattern, leading to poor generalisation to unseen data. Regularisation combats this by constraining the model’s flexibility, typically through penalising large coefficients in the objective function.

\subsubsection*{Linear Regression and Overfitting}

In ordinary least squares (OLS) linear regression, the model assumes a linear relationship between the features $x_i$ and the target variable $y$:
\[
\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n = X\boldsymbol{\theta}
\]
Here, $X$ is the feature matrix (with a bias column of 1s), and $\boldsymbol{\theta}$ is the vector of model coefficients. The goal is to minimise the mean squared error (MSE):
\[
\mathcal{L}_{\text{OLS}} = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2
\]
However, in high-dimensional or noisy data, OLS can lead to large coefficient magnitudes, capturing noise and resulting in poor generalisation.

\subsubsection*{The Regularised Cost Function}

To address this, regularisation modifies the cost function by adding a penalty term that constrains the size of the coefficients:
\[
\mathcal{L}_{\text{reg}} = \text{MSE} + \lambda \cdot \text{Penalty}(\boldsymbol{\theta})
\]
The hyperparameter $\lambda$ controls the trade-off between fitting the data and keeping coefficients small.

\subsubsection*{Ridge and Lasso Regression}

Two of the most common regularisation methods are:

\paragraph{Ridge Regression (L2 penalty):}
\[
\mathcal{L}_{\text{ridge}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^n \theta_j^2
\]
The L2 penalty shrinks all coefficients smoothly toward zero. It is especially effective in scenarios where all features contribute a little.

\paragraph{Lasso Regression (L1 penalty):}
\[
\mathcal{L}_{\text{lasso}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^n |\theta_j|
\]
The L1 penalty can shrink some coefficients exactly to zero, effectively performing \emph{feature selection}. This sparsity makes lasso particularly useful when the number of features is large and only a subset is expected to be informative.

\subsubsection*{Performance in Different Noise Environments}

The effectiveness of each method varies by signal-to-noise ratio (SNR) and the sparsity of the true coefficients:

\begin{itemize}
    \item \textbf{High SNR, sparse coefficients:} Lasso excels by zeroing out irrelevant features while accurately identifying important ones.
    \item \textbf{Low SNR, sparse coefficients:} Lasso remains robust, outperforming ridge and linear regression, which tend to overfit.
    \item \textbf{High SNR, non-sparse coefficients:} All methods perform reasonably well; lasso identifies some zeros, while ridge is slightly more stable.
    \item \textbf{Low SNR, non-sparse coefficients:} Ridge generally outperforms lasso in fitting non-zero coefficients, but lasso still performs well as a feature selector.
\end{itemize}

\subsubsection*{Empirical Comparison}

Empirical plots comparing test predictions to actual values show that:
\begin{itemize}
    \item \textbf{Lasso regression} tracks actual values closely with predictions concentrated around the ideal diagonal.
    \item \textbf{Ridge regression} performs better than standard linear regression in noisy settings.
    \item \textbf{Linear regression} often overfits, especially in low SNR environments, resulting in erratic coefficient estimates and poor generalisation.
\end{itemize}

In a moderately noisy setting, lasso achieves significantly lower MSE than ridge and linear regression—up to 30 times smaller. This demonstrates the power of regularisation in producing generalisable models.

\subsubsection*{Regularisation in Classification}

Regularisation principles extend naturally to classification models, such as logistic regression. The cost function, often based on cross-entropy, is augmented by L1 or L2 penalties to prevent the model from overfitting the training data:
\[
\mathcal{L}_{\text{logistic-reg}} = \text{CrossEntropy}(\hat{y}, y) + \lambda \cdot \text{Penalty}(\boldsymbol{\theta})
\]
This is particularly useful in high-dimensional spaces, such as text classification, where sparsity and noise are common.
\subsection{Data Leakage}

Data leakage is a critical issue that arises when information from outside the 
training dataset, typically unavailable during deployment—leaks into the model training process. This leads to artificially inflated model performance during validation and testing, but poor generalisation in real-world applications.

\subsubsection*{What is Data Leakage?}

Data leakage occurs when the training data includes information that would not be available at prediction time. For example, using a feature engineered from the entire dataset (e.g., the average home price) when predicting individual house prices introduces information from the test set into the training process. This scenario mimics having access to the future, which is unrealistic in production.

\textbf{Data snooping} is a specific form of data leakage where information from the test set indirectly influences the training process, such as tuning hyperparameters based on test set performance or feature engineering using future labels.

\subsubsection*{Consequences of Leakage}

When data leakage is present:
\begin{itemize}
    \item The model appears to perform well on the test set.
    \item Model evaluation metrics are misleading.
    \item The deployed model underperforms, failing to replicate evaluation results.
\end{itemize}

Leakage invalidates the model validation pipeline, making the trained model unreliable.

\subsubsection*{Common Sources of Data Leakage}

\begin{itemize}
    \item \textbf{Global statistics:} Using dataset-level aggregates (e.g., overall mean, standard deviation) during training.
    \item \textbf{Temporal leakage:} Using future data to predict past or present values.
    \item \textbf{Inappropriate feature engineering:} Deriving features from the entire dataset before splitting into training/test sets.
    \item \textbf{Improper cross-validation:} Applying preprocessing steps (e.g., scaling, PCA) before the data is split into folds.
\end{itemize}

\subsubsection*{Preventing Data Leakage}

To mitigate the risk of leakage:
\begin{itemize}
    \item Keep training, validation, and test sets strictly separate with no contamination.
    \item Ensure feature engineering and preprocessing are performed within a pipeline fitted only on the training data.
    \item Avoid global statistics computed using the full dataset.
    \item For time-dependent data, use time-aware splits (e.g., \texttt{TimeSeriesSplit}) rather than random train-test splits.
    \item During cross-validation, fit the pipeline separately within each fold to avoid leakage across folds.
\end{itemize}



\end{document}